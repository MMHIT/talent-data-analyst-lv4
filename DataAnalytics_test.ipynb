{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eec3119",
   "metadata": {},
   "source": [
    "# ST IT Cloud - Data and Analytics Test LV.4\n",
    "\n",
    "Esse teste deve avaliar alguns conceitos de big data e a qualidade técnica na manipulacão de dados, otimização de performance, trabalho com arquivos grandes e tratamento de qualidade.\n",
    "\n",
    "## Passo a passo\n",
    "\n",
    "- *Parte teórica:* responda as questões abaixo preenchendo as células em branco.\n",
    "- *Parte prática:* disponibilizamos aqui 2 cases para, leia os enunciados dos problemas, desenvolver os programas, utilizando a **stack definida durante o processo seletivo**, para entregar os dados de acordo com os requisitos descritos abaixo.\n",
    "\n",
    "\n",
    "\n",
    "**Faz parte dos critérios de avaliacão a pontualidade da entrega. Implemente até onde for possível dentro do prazo acordado.**\n",
    "\n",
    "**Os dados de pessoas foram gerados de forma aleatória, utilizando a biblioteca FakerJS, FakerJS-BR e Faker**\n",
    "\n",
    "LEMBRE-SE: A entrega deve conter TODOS os passos para o avaliador executar o programa (keep it simple).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447dec4",
   "metadata": {},
   "source": [
    "**Questão 1** - Descreva de forma detalhada quais são as etapas na construção de um pipeline de dados, sem considerar ferramentas específicas, imagine que é seu primeiro contato com o cliente e você precisa entender a demanda dele e explicar quais são os passos que você terá que implementar para entregar a demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Início de um pipeline devemos definir quais serão nossos data sources(de onde os dados vem), \n",
    "# em seguida, qual a estrutura desses dados(estruturados, semiestruturados ou não estruturados). \n",
    "\n",
    "# Definidas essas características iniciamos a camada de processamento, com a primeira etapa de: \n",
    "# retenção(gralmente life cycle), controle(geralmente cdc) e formato(geralmente compactado). \n",
    "\n",
    "# Em sequência passamos pra tratamento e validação, onde implementamos um data quality e definimos \n",
    "# as necessidade dos dados para que se tornem estruturados(geralmente validadções quanto a integridade e governança) \n",
    "# até que por fim, geralmente passamos pela fase de enriquecimento, onde usamos os dados para gerar visões, dimensões e fatos que \n",
    "# facilitam e aprimoram as análises futuras(é possível criar dados generalizados que enriqueçam a partir de uma união(denominada flat table)). \n",
    "\n",
    "# Após essas 3 camadas definimos os data targets(quem se alimentará desses dados). \n",
    "\n",
    "# Obs: A descrição anterior é referente ao que conhecemos na engenharia como ETL e pode ser implementado mais recentemente no formato ELT. \n",
    "# Data source e data target a depender do cliente definem o formato da arquitetura, podendo ser do tipo(DL, DW, Lambda ou Kappa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc703af3",
   "metadata": {},
   "source": [
    "**Questão 2** - Defina com suas palavras um processamento em streaming e processamento em batch. Qual sua experiência com cada uma delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming - processamento de dados em fluxo contínuo, geralmente utilizado para análise em tempo real(ou quase) onde a coleta e entrega acontecem quase no mesmo instante. (Tenho pouca experiência prática, estudei algumas arquitetura e estudo por conta algumas ferramentas aws e open source). \n",
    "\n",
    "# Batch - processamento de dados que implica em um período de coleta(geralmente 1 dia ou menos). (Toda minha experiência foi forjada com arquitetura em batch, onde já implementei com período de reteção de 1 dia, 8h, 30 min e 15 min). Obs: períodos abaixo de 30 min já são considerados New Real-Time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c3f10",
   "metadata": {},
   "source": [
    "**Questão 3** - Quais são as camadas de um Data Lake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b803419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possuem normalmente de 3 a 4 camadas onde os nomes são diversos, porém academicamente chamados de  Landing-zone, (Bronze, Silver e Gold) ou (Raw, Trusted e Refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62477089",
   "metadata": {},
   "source": [
    "**Questão 4** - Quais as diferenças de um Data Lake e um DW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suas principais diferenças estão em que tipo de dado armazenam e a forma como são implementados.\n",
    "# DW - em um data Warehouse definimos primeiramente um esquema de dados edepois armazenamos esses dados. O que implica dizer que o dw trabalha apenas com dados estruturados.\n",
    "# DL - Em um data Lake é o contrário, primeiro armazenamos qualquer tipo dados e no momento da leitura é que definimos o tipo de dado. o que implica dizer que ele aceita dados não estruturados, estruturados e semiestruturados "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099a05e",
   "metadata": {},
   "source": [
    "**Questão 5** - O que é arquitetura Lambda e Kappa? Descreva com suas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4347923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda - é uma arquitetura que executa o processamento de dados em diferentes camadas com foco no streaming em lotes. ou seja, é a junção de batch, serving e speed layers. \n",
    "\n",
    "# Kappa - é uma simplificação da lambda focando no processo de streaming, removendo a etapa em lotes e fazendo com que o fluxo flua 100% streaming. o que acarreta na utilização de uma camada Real_time e uma serving layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d483e",
   "metadata": {},
   "source": [
    "**Questão 6** - O que é Data Quality para você e como você implementa isso nos seus processos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality é o processo pelo qual atribuímos aos dados relevância qualitativa ou quantitativa no que é esperado para a análise.\n",
    "# Em meus processos essa etapa se localiza entre as camadas de RAW e Trusted, onde o processamento será responsável por organizar, validar e corrigir os dados. Em outras palavras tornando-os estruturados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4834d6",
   "metadata": {},
   "source": [
    "**Questão 7** - Em uma escala de 0 a 10, qual seria seu nível de experiência com PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meu Pyspark é 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef78ba",
   "metadata": {},
   "source": [
    "**Questão 8** - Em uma escala de 0 a 10, qual seria seu nível de experiência com SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33569bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meu SQL é 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f7ee6",
   "metadata": {},
   "source": [
    "**Questão 9** - Descreva suas expeciências com banco de dados SQL e NoSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sql: de modelagem relacional a squematização e manutenção. trabalhei com dba em postgres 3 anos atrás e de lá pra cá já atuei com Postgres, MySql e SqlServer.\n",
    "# NoSql: tive uma atuação mais forte dentro da aws com DynamoDB, criando e dando manutenção em tabelas e cluster. Atuei também com Mongo, porem muito mais teórico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fa6e5",
   "metadata": {},
   "source": [
    "**Questão 10** - Tem experiência com versionamento de código? Com quais ferramentas já trabalhou? Descreva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenho sim, já trabalhei com git, gitlab e bitbucket, como dev, devops e dataops faço uso constante de versionamento de código.\n",
    "# principalmente pro trabalhar com IAAC, toda a arquitetura é tratada via script terraform e para elém disso os códigos internos das etapas do lake também são versionadas e adicionadas a um storage em cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2714f",
   "metadata": {},
   "source": [
    "**Questão 11** - Tem experiência em desenvolvimento em cloud? Se sim, especifique a(s) plataforma(s) que já trabalhou e suas principais implementações e conhecimentos em cada serviço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualmente trabalho apenas com aws, Atuei em cloud como analista e como engenheiro, desde todos os serviços básicos até os mais específicos.\n",
    "\n",
    "# serviços básicos: ec2, rds, s3, mds, efs, fsx, route53, lambda, ELB e NLB, elatic ip etc...\n",
    "\n",
    "# serviços de engenharia: EMR, pacote glue(Catalog, crawler, job), Athena, Redshift, ecs, ecr, pacote codePipeline(Codecommit, codebuild e coddeploy) etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bc70d",
   "metadata": {},
   "source": [
    "**Questão 12** - Tem experiência com metodologia ágil? Qual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5be8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sim, Scrum basicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb61f06",
   "metadata": {},
   "source": [
    "# TESTE PRÁTICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8c6a5",
   "metadata": {},
   "source": [
    "**Problema 1**: Você está recebendo o arquivo 'dados_cadastrais_fake.csv' que contem dados cadastrais de clientes, mas para que análises ou relatórios sejam feitos é necessário limpar e normalizar os dados. Além disso, existe uma coluna com o número de cpf e outra com cnpj, você precisará padronizar deixando apenas dígitos em formato string (sem caracteres especiais), implementar uma forma de verificar se tais documentos são válidos sendo que a informação deve se adicionada ao dataframe em outras duas novas colunas.\n",
    "\n",
    "Após a normalização, gere reports que respondam as seguintes perguntas:\n",
    "- Quantos clientes temos nessa base?\n",
    "- Qual a média de idade dos clientes?\n",
    "- Quantos clientes nessa base pertencem a cada estado?\n",
    "- Quantos CPFs válidos e inválidos foram encontrados?\n",
    "- Quantos CNPJs válidos e inválidos foram encontrados?\n",
    "\n",
    "Ao final gere um arquivo no formato csv e um outro arquivo no formato parquet chamado (problema1_normalizado), eles serão destinados para pessoas distintas.\n",
    "\n",
    "*EXTRA:* executar as mesmas validações no *1E8.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b6c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2da9c40",
   "metadata": {},
   "source": [
    "**Problema 2**: Você deverá implementar um programa, para ler, tratar e particionar os dados.\n",
    "\n",
    "O arquivo fonte está disponível em `https://st-it-cloud-public.s3.amazonaws.com/people-v2_1E6.csv.gz`\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- Higienizar e homogenizar o formato da coluna `document`\n",
    "- Detectar através da coluna `document` se o registro é de uma Pessoa Física ou Pessoa Jurídica, adicionando uma coluna com essa informação\n",
    "- Higienizar e homogenizar o formato da coluna `birthDate`\n",
    "- Existem duas colunas nesse dataset que em alguns registros estão trocadas. Quais são essas colunas? \n",
    "- Corrigir os dados com as colunas trocadas\n",
    "- Além desses pontos, existem outras tratamentos para homogenizar esse dataset. Aplique todos que conseguir.\n",
    "\n",
    "### Agregação dos dados\n",
    "\n",
    "- Quais são as 5 PF que mais gastaram (`totalSpent`)? \n",
    "- Qual é o valor de gasto médio por estado (`state`)?\n",
    "- Qual é o valor de gasto médio por `jobArea`?\n",
    "- Qual é a PF que gastou menos (`totalSpent`)?\n",
    "- Quantos nomes e documentos repetidos existem nesse dataset?\n",
    "- Quantas linhas existem nesse dataset?\n",
    "\n",
    "### Particionamento de dados tratados com as regras descritas em `DATA QUALITY`\n",
    "\n",
    "- Particionar em arquivos PARQUET por estado (`state`)\n",
    "- Particionar em arquivos CSV por ano/mes/dia de nascimento (`birthDate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277f816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
